import numpy as np
from scipy import stats
import json
import argparse
import re

def safe_mode(arr):
    """Safely calculate the mode, handling single values or empty arrays."""
    if len(arr) == 0:
        return None  # No values to compute mode from

    mode_result = stats.mode(arr)

    # Check if mode_result is scalar or array and return the correct value
    if isinstance(mode_result.mode, np.ndarray) and len(mode_result.mode) > 0:
        return mode_result.mode[0]
    elif isinstance(mode_result.mode, (int, float, np.float64, np.int64)):
        return mode_result.mode  # If it's a scalar
    else:
        return None

def convert_numpy_types(obj):
    """Recursively convert NumPy types to Python native types."""
    if isinstance(obj, np.ndarray):
        return obj.tolist()  # Convert NumPy arrays to lists
    elif isinstance(obj, (np.int64, np.float64, np.int32, np.float32)):
        return obj.item()  # Convert NumPy scalars to Python scalars
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

def generate_final_report(results):
    """Generate a report with mean, median, and mode Jaccard and Cosine similarities across documents for each OCR method."""
    
    # Dictionary to hold aggregated scores for each method across all documents
    method_scores = {}
    method_name_pattern = re.compile(r"(pytesseract_text|corrected_by_[\w-]+)")

    # Aggregate the results by method across all documents
    for document_id, document_data in results.items():

        for full_method_name, scores in document_data.items():

            # Use regex to find the base method name
            match = method_name_pattern.search(full_method_name)
            if match:
                base_method_name = match.group(0)  # Extract the matched base method name
            else:
                continue  # Skip if method name pattern is not matched

            if base_method_name not in method_scores:
                method_scores[base_method_name] = {
                    "jaccard_scores": [],
                    "cosine_scores": [],
                    "match_percentage": [],
                    "fuzzy_matched_jaccard": []
                }

            method_scores[base_method_name]["jaccard_scores"].append(scores["jaccard_similarity"])
            method_scores[base_method_name]["cosine_scores"].append(scores["avg_cosine_similarity"])
            method_scores[base_method_name]["match_percentage"].append(scores["match_percentage"])
            method_scores[base_method_name]["fuzzy_matched_jaccard"].append(scores["fuzzy_matched_jaccard"])
    # Generate the final report with mean, median, and mode for each method across documents
    final_report = {}
    for method, scores in method_scores.items():
        jaccard_scores = np.array(scores["jaccard_scores"])
        cosine_scores = np.array(scores["cosine_scores"])
        match_percentages = np.array(scores["match_percentage"])
        fuzzy_matched_jaccard = np.array(scores["fuzzy_matched_jaccard"])
        final_report[method] = {
            "jaccard_similarity": {
                "mean": np.mean(jaccard_scores) if len(jaccard_scores) > 0 else None,
                "median": np.median(jaccard_scores) if len(jaccard_scores) > 0 else None,
                "mode": safe_mode(jaccard_scores)  # Call safe_mode
            },
            "cosine_similarity": {
                "mean": np.mean(cosine_scores) if len(cosine_scores) > 0 else None,
                "median": np.median(cosine_scores) if len(cosine_scores) > 0 else None,
                "mode": safe_mode(cosine_scores)  # Call safe_mode
            },
            "match_percentage": {  # New section for match percentage
                "mean": np.mean(match_percentages) if len(match_percentages) > 0 else None,
                "median": np.median(match_percentages) if len(match_percentages) > 0 else None,
                "mode": safe_mode(match_percentages)
            },
            "fuzzy_matched_jaccard": {  # New section for match percentage                                                                                                             
                "mean": np.mean(fuzzy_matched_jaccard) if len(fuzzy_matched_jaccard) > 0 else None,
                "median": np.median(fuzzy_matched_jaccard) if len(fuzzy_matched_jaccard) > 0 else None,
                "mode": safe_mode(fuzzy_matched_jaccard)
            }
        }

    return final_report

def main_report(args):
    # Load the results JSON file generated by the previous comparison process
    with open(args.results_file, 'r') as f:
        results = json.load(f)

    # Generate the final report with aggregated statistics across all documents
    report = generate_final_report(results)

    # Convert any NumPy types to native Python types before saving to JSON
    report = convert_numpy_types(report)

    # Output the final report as a JSON file
    with open(args.output_report_file, 'w') as f_out:
        json.dump(report, f_out, indent=4)

# Argument parser setup for final report generation
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate final report with aggregated statistics.")
    parser.add_argument("--input", dest='results_file', type=str, help="Path to JSON file with OCR comparison results.")
    parser.add_argument("--output", dest='output_report_file', type=str, help="Path to output JSON file for the final report.")

    args = parser.parse_args()
    main_report(args)
